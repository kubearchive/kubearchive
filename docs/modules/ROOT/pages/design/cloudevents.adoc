= Cloud Event Delivery Pipeline
== Overview

KubeArchive uses Cloud Events that are generated by a Knative Eventing ApiServerSource to discover when resources on
the kubernetes cluster have changed. These Cloud Events contain the json representation of the Kubernetes resource that
has changed. The KubeArchive Sink (sink) receives these messages and determines, using CEL expression from the
KubeArchiveConfig, if the data contained in the Cloud Event should be written to the database and potentially deleted
from the cluster. While the sink is processing a Cloud Event, it is possible at many points for an error to occur,
especially when the sink has to make a request of the network, particularly, to the Kubernetes API or the database. This
page proposes how KubeArchive set up its Cloud Event delivery pipeline and how it should consume Cloud Events to achieve
the following goals:

* KubeArchive receives all Cloud Events from the Knative ApiServerSource
* Cloud Events sent to KubeArchive can be replayed
* Cloud Events received by KubeArchive, that cannot be processed successfully, are reported back to the sender as
  delivery failure
* Cloud Events sent to the sink will attempt to be redelivered if the initial delivery fails
* Cloud Events that could not be delivered are replayed to KubeArchive automatically when KubeArchive becomes available
  again

== Design

KubeArchive needs to use the functionality provided by Knative Eventing, Brokers and Triggers as well as features
provided by the Cloud Events Specification to achieve an event driven architecture, where events are not lost and events
can be replayed safely without worries about side effects.

This diagram provides a high level overview of the life cycle of Cloud Events look like in KubeArchive and how the
different components of the Cloud Event Delivery Pipeline interact with eachother.

image::ce-pipeline.png[]

=== Knative Eventing Brokers

Instead of the ApiServerSource sending Cloud Events directly to the sink, the Cloud Events should instead be sent to a
Knative Eventing Broker. Brokers can receive, persist, and send messages. Using the broker allows messages to be
sent somewhere that they can be stored long term and is separate from the KubeArchive sink and Knative Eventing
ApiServerSource, so if either are restarted, the Cloud Events won't be lost. KubeArchive will use two brokers. One
primary broker (called broker) and secondary broker (called dead letter sink). The default configuration of
Knative Eventing will have these brokers be a created as In Memory Brokers, but changing the Knative Eventing
configuration can allow this broker to be created, for example, as a Kafka Broker instead

[IMPORTANT]
====
For Production workloads, Knative Eventing strongly recommends using Brokers types other than the In Memory Broker.
====

==== Primary Broker

The primary broker receives all of Cloud Events from the ApiServerSources created by the KubeArchive Operator.
KubeArchive uses a trigger to subscribe to Cloud Events from this broker. The primary broker is configured as follows:

[source,yaml]
----
spec:
  delivery:
    retry: 4
    backoffPolicy: exponential
    # ISO 8601 format
    backoffDelay: PT0.5S
    deadLetterSink:
      ref:
        apiVersion: eventing.knative.dev/v1
        kind: Broker
        name: kubearchive-dls
----

If the initial delivery fails, the broker will attempt to deliver the cloud event 4 additional times. When attempting to
retry delivering the event, the broker will use an exponential backoff algorithm to calculate the amount of time to wait
between retry attempts, and if all retry attempts fail, the broker will send the cloud event to the dead letter sink.

==== Dead Letter Sink

The dead letter sink is a second broker that stores cloud events that could not be delivered to the KubeArchive sink. If
the sink has received every message from the broker, then there will be no cloud events stored in the dead letter sink.
If the sink becomes unreachable or unable to process cloud events, the dead letter sink will start to receive cloud
events from the primary broker.

The sink needs to be able to receive cloud events from the dead letter sink after recovering from a error state, but the
process for doing this is still TBD.

=== Triggers

KubeArchive creates a trigger that subscribes the KubeArchive sink to all cloud events stored by the primary broker. It
is configured as follows:

[source,yaml]
----
spec:
  broker: kubearchive-broker
  subscriber:
    ref:
      apiVersion: v1
      kind: Service
      name: kubearchive-sink
----

This configuration tells the primary broker to send cloud events to the url for the `kubearchive-sink` service on the
`/` route using the `HTTP POST` method.

KubeArchive will need to create a trigger so the sink can subscribe to cloud events from the dead letter sink, but how
this will be configured is TBD.

=== HTTP Status Codes

The sink receives cloud events from Knative Eventing over HTTP on the `/` route using the `HTTP POST` method. After
receiving the cloud event, the sink has to process the cloud event to determine if the resource contained in the cloud
event needs to be written to the database and if the resource needs to be deleted from the cluster. All cloud events
must be successfully processed for KubeArchive to function as intended. To make sure that this happens, KubeArchive will
respond to cloud events with the following HTTP response codes:

[cols="1,1"]
|===
|HTTP Status Code |Meaning

|200
|successfully received and processed the cloud event

|422
|successfully received a valid cloud event, but the `data` field could not be unmarshalled into a Kubernetes object. Do
not send the cloud event again without modifying the `data` field first.

|500
|successfully received a valid cloud event with a kubernetes resource in it, but kubearchive encoutered an error while
trying to process the resource. This can happen if there was issue writing to the database or requesting the Kubernetes
API to delete an object. The cloud event can be sent again without modifying.
