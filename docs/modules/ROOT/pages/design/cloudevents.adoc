= Cloud Event Delivery Pipeline
== Overview

KubeArchive uses Cloud Events that are generated by a Knative Eventing ApiServerSource to discover when resources on
the kubernetes cluster have changed. These Cloud Events contain the JSON representation of the Kubernetes resource that
has changed. The KubeArchive Sink (sink) receives these messages and determines, using CEL expression from the
KubeArchiveConfig, if the data contained in the Cloud Event should be written to the database and potentially deleted
from the cluster. While the sink is processing a Cloud Event, it is possible at many points for an error to occur,
especially when the sink has to make a request of the network, particularly, to the Kubernetes API or the database. This
page proposes how KubeArchive set up its Cloud Event delivery pipeline and how it should consume Cloud Events to achieve
the following goals:

* Cloud Events sent to KubeArchive can be replayed
* Cloud Events received by KubeArchive, that cannot be processed successfully, are reported back to the sender as
  delivery failure
* Cloud Events sent to the sink will attempt to be redelivered if the initial delivery fails
* Cloud Events that could not be delivered are replayed to KubeArchive automatically when KubeArchive becomes available
  again (WIP)

== Design

We decided to use an Event Driven architecture for KubeArchive. By reusing Knative Eventing components, KubeArchive can
be fault tolerant and scale well with multiple event consumers.

This diagram provides a high level overview of the life cycle of Cloud Events look like in KubeArchive and how the
different components of the Cloud Event Delivery Pipeline interact with eachother.

image::ce-pipeline.png[]

=== Knative Eventing Brokers

Instead of sending events directly to KubeArchive's sink, we decided to send the events to a Knative Eventing
Broker. These can persist events in case of consumer or producer failure. By default KubeArchive uses two in-memory
brokers: primary and dead letter sink. The primary broker receives the events from the ApiServerSource and sends them to
the dead letter sink broker if its unable to deliver them to KubeArchive.

[IMPORTANT]
====
For production workloads, Knative Eventing recommends using Broker types other than the In Memory Broker.
====

==== Primary Broker

The primary broker receives all of Cloud Events from the ApiServerSources created by the KubeArchive Operator.
KubeArchive uses a trigger to subscribe to Cloud Events from this broker. The primary broker is configured as follows:

[source,yaml]
----
spec:
  delivery:
    retry: 4
    backoffPolicy: exponential
    # ISO 8601 format
    backoffDelay: PT0.5S
    deadLetterSink:
      ref:
        apiVersion: eventing.knative.dev/v1
        kind: Broker
        name: kubearchive-dls
----

If the initial delivery fails, the broker will resend the Cloud Event using the configured `retry`, `backoffPolicy`, and
`backoffDelay` parameters. These parameters can be changed by the user, see the
link:https://knative.dev/docs/eventing/event-delivery/#configuring-broker-event-delivery[Knative documentation]
for details. If all retry attempts fail, the broker will send the cloud event to the dead letter sink.

==== Dead Letter Sink

The dead letter sink stores Cloud Events that could not be delivered to the KubeArchive sink. This includes Cloud Events
that KubeArchive received, was unable to process successfully. See the <<_http_status_codes>> section for details.

The KubeArchive sink needs to be able to receive cloud events from the dead letter sink after recovering from an error
state, but the process for doing this is still TBD.

=== Triggers

KubeArchive creates a trigger that subscribes the KubeArchive sink to all cloud events stored by the primary broker. It
is configured as follows:

[source,yaml]
----
spec:
  broker: kubearchive-broker
  subscriber:
    ref:
      apiVersion: v1
      kind: Service
      name: kubearchive-sink
----

This configuration tells the primary broker to target the `kubearchive-sink` Service. By default events are sent to the
root endpoint (`/`) over HTTP using the `POST` method.

KubeArchive will need to create a trigger so the sink can subscribe to cloud events from the dead letter sink, but how
this will be configured is TBD.

[_http_status_codes]
=== HTTP Status Codes

The sink receives Cloud Events from the ApiServerSource over HTTP on the `/` route using the `POST` method. After
receiving the event, the sink processes it to determine if the resource change needs to be written to the database and
if the resource needs to be deleted from the cluster. The different HTTP response codes KubeArchive can send in
response to a `POST` request have different meanings:

[cols="1,1"]
|===
|HTTP Status Code |Meaning

|200
|successfully received and processed the cloud event

|422
|successfully received a valid cloud event, but the `data` field could not be unmarshalled into a Kubernetes object. Do
not send the cloud event again without modifying the `data` field first.

|500
|successfully received a valid cloud event with a kubernetes resource in it, but kubearchive encountered an error while
trying to process the resource. This can happen if there was issue writing to the database or requesting the Kubernetes
API to delete an object. The cloud event can be sent again without modification.
